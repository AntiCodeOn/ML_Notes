\documentclass[11pt]{article}
\renewcommand{\baselinestretch}{1.05}
\usepackage{amsmath,amsthm,verbatim,amssymb,amsfonts,amscd, graphicx}
\usepackage{bm}
\usepackage{graphics}
\usepackage{cite}

\DeclareMathOperator*{\argminA}{arg\,min}



\begin{document}



\title{Expectation Maximization with Coin Toss example}
\author{AntiCodeOn}
\maketitle
\section{Coin toss problem description}
This documents contains detailed solution for the problem described in ~\cite{nature}. In summary, someone choose randomly one of two biased coins five times. After each random selection, the given coin has been tossed ten times and outcomes were recorded. We were given the outcomes of these five events but not the identities of coins whom each event belongs. Also, coin biases are unknown. Our goal is to somehow infer these parameters from the given data. In order to solve the problem we resort to the Expectation Maximization (EM) algorithm. 

\section{Theoretical background of EM}

Given the statistical model which generates a set \textbf{X} of observed data, a set of unobserved latent data or missing values \textbf{Z}, and a vector of unknown parameters \textbf{$\Theta$}, along with a likelihood function \boldmath$\textit{L}(\Theta;X,Z) = \textit{p}(X,Z|\Theta)$,
the maximum likelihood estimate (MLE) of the unknown parameters is determined by the marginal likelihood of the observed data

\begin{equation}
\textit{L}(\Theta;X)=\textit{p}(X|\Theta)=\int\textit{p}(X,Z|\Theta)\textit{d}Z
\end{equation}
The EM algorithm seeks to find the MLE of the marginal likelihood by iteratively applying these two steps:

\textit{Expectation step (E step):} Calculate the expected value of the log likelihood function, with respect to the conditional distribution of \textbf{Z} given \textbf{X} under the current estimate of the parameters $\Theta^{(t)}$:

\begin{equation}\label{exstep}
Q(\Theta|\Theta^{(t)})=E_{Z|X,\Theta^{(t)}}[log\textit{L}(\Theta;X,Z)]    
\end{equation}

\textit{Maximization step (M step):} Find the parameter that maximizes this quantity:
\begin{equation}\label{maxstep}
\Theta^{(t+1)}=\argminA_\Theta Q(\Theta|\Theta^{(t)})
\end{equation}
This is a word by word definition\footnote{I choose to take this particular definition (out of many out there) because it is the first one I actually understood. I don't think that this is a better or worse than the others, it's just that my brain was finally ready.} copied from [1].
If you don't understand what the above means at first, don't worry. It is merely due to the compactness of the mathematical language. The first term (Equation \eqref{exstep}) means: Set up the model for the likelihood function pretending that you know what is the value of $\Theta$. In the actual solution to the coin toss problem, we randomly instantiate its values to $\Theta_i \in [0,1]$ where \textit{i} is the i-th component of the vector $\Theta$, i.e. bias of the i-th coin. Once we have defined this equation (model) we take the expectation over it to find the probability of each possible value of $Z$, given $\Theta$. Then we compute a better estimate for the parameters $\Theta$ using these probabilities (Equation \eqref{maxstep}) . We iterate these procedure until convergence.
\subsection{Problem setup}\label{section:mathmode}
Vector \textbf{z} corresponds to the event of choosing one of the two given coins. It is bivariate, the values of each of the two possible outcomes are mutually exclusive.
\begin{equation}
\bm{z_{n}} = \begin{bmatrix}
z_{n1} \\
z_{n2}
\end{bmatrix}
\in \left \{
\begin{bmatrix}
{1} \\
{0}
\end{bmatrix},
\begin{bmatrix}
{0} \\
{1}
\end{bmatrix}
\right \}
\end{equation}

As $z$ uses 1-of-K representation we can write probability distribution of the event $z_n$ (which coin we selected in the n-th event) in a following way
\begin{equation}\label{oneofk}
p(z_{n}) = \prod_{k=1}^{2} \pi_{k}^{z_{nk}}
\end{equation}
with requirement that $\sum_{k}\pi_{k}$ = 1.

Each single coin toss is independent on the other tosses. It only depends on the coin we choose previously. Probability of the single coin toss outcome given coin k is governed by Bernoulli distribution
\begin{equation}
p(x_n^j|{z_n},\Theta)=\prod_{k=1}^{2}[\Theta_k^{x_n^j}(1-\Theta_k^{1-x_n^j})]^{z_{nk}}
\end{equation}
where $x_n^j=1$ if j-th outcome was head and $x_n^j=0$ if j-th outcome was tail. We can now determine the probability of each of our 5 events
\begin{equation}\label{jointbern}
p(\{x^1_n\cdots x^{10}_n\}|z_n,\Theta)=\prod_{j=1}^{10}p(x_n^j|{z_n},\Theta)
\end{equation}

Joint probability of the coin toss outcomes and coin selections is given by:
\begin{equation}
P = p(X_1,\dots, X_5,{z_1,\dots,z_5}|\Theta) 
\end{equation}
%\begin{equation}
%= p(\{x^1_1\cdots x^{10}_1\},\dots,\{x^1_5\cdots x^{10}_5\}, z_1,\dots,z_5|\Theta)
%\end{equation}
\begin{equation}
= p(\{x^1_1\cdots x^{10}_1\},\dots,\{x^1_5\cdots x^{10}_5\}| z_1,\dots,z_5,\Theta)p(z_1,\dots,z_5)
\end{equation}
\begin{equation}\label{jointxz}
=\prod_{n=1}^{5}p(\{x^1_n\cdots x^{10}_n\}|z_n,\Theta) \prod_{n=1}^{5}p({z_n})
\end{equation}
Finally, combining the \eqref{oneofk}, \eqref{jointbern} and \eqref{jointxz} we get the equation
\begin{equation}
P = 
\prod_{n=1}^{5}\prod_{k=1}^{2}\prod_{j=1}^{10}[\Theta_k^{x_n^j}(1-\Theta_k^{1-x_n^j})]^{z_{nk}}\:\pi_{k}^{z_{nk}}
\end{equation}
Taking the log of the expression above we get rid of the products in exchange of the summations.
\begin{equation}
log(P) = 
\sum_{n=1}^{5}\sum_{k=1}^{2}z_{nk}[\sum_{j=1}^{10}log(\Theta_k^{x_n^j}(1-\Theta_k^{1-x_n^j}))+log(\pi_{k})]
\end{equation}
The final step is taking the expectation over hidden random variable $z$. In this case, we are using the conditional expectation, as we do not
know what was the outcome on each of the step. Using the linearity of the expectation as
\begin{equation}
E_{Z|X}[log(P)] = 
\sum_{n=1}^{5}\sum_{k=1}^{2}E_{Z|X}[z_{nk}][\sum_{j=1}^{10}log(\Theta_k^{x_n^j}(1-\Theta_k^{1-x_n^j}))+log(\pi_{k})]
\end{equation}
Conditional expectation gives us the expected value of the random experiment in light of the
new information on which we are conditioning. Consider throwing an ordinary dice. The expected value of the outcome is $E(number-on-dice)=(1+2+3+4+5+6)/6=3.5$. But if I told you the parity of the outcome, in that case we would calculate the expectation $E(odd)=(1+3+5)/3=3$ and $E(even)=(2+4+6)/3=4$. We see that it is not enough to express the expectation with the one value only. We can use similar analogy in our experiment. As we have see the outcomes of each of the five events, we will assign bigger value of $\Theta$ to the ones that have had more heads. If there isn't any significant difference in the outcomes we will have a hard time trying to figure out which event belongs to which coin.
\begin{equation}
  E_{Z|X}[z_{nk}]=
\sum_{z}z_{nk}p(z_n|\{x^1_n\cdots x^{10}_n\};\Theta)  
\end{equation}

Using the Bayes formula, the posterior of $z_n$ given values of the n-th event is
\begin{equation}
p(z_n|\{x^1_n\cdots x^{10}_n\};\Theta)=\frac{p(\{x^1_n\cdots x^{10}_n\}|z_n;\Theta)p(z_n)}{p(\{x^1_n\cdots x^{10}_n\};\Theta)}
\end{equation}
We already know the expressions for the terms in numerator, and the expression in denominator is given by marginalizing over all possible outcomes of $z$ (in our case there are only two, as already mentioned). 
\begin{equation}
p(z_n|x_n;\Theta)=\frac{\prod_{k=1}^{2}[\pi_{k}p(x_n;\Theta_k)]^{z_{nk}}}{\sum_{z_m}\prod_{m=1}^{2}[\pi_{m}p(x_n;\Theta_m)]^{z_{nm}}}
\end{equation}
Going back to our expectation formula
\begin{equation}
E_{Z|X}[z_{nk}]=
\frac{\sum_{z_n}z_{nk}\prod_{k=1}^{2}[\pi_{k}p(x_n;\Theta_k)]^{z_{nk}}}{\sum_{z_m}\prod_{m=1}^{2}[\pi_{m}p(x_n;\Theta_m)]^{z_{nm}}}
\end{equation}
We see that in the numerator those events where $z_{nk}=0$ will be eliminated, while in the denominator we are still summing over all possibilities. 
\begin{equation}
E_{Z|X}[z_{nk}]=
\frac{\pi_{k}p(x_n;\Theta)}{\sum_{m=1}^{2}\pi_{m}p(x_n;\Theta_m)}
\end{equation}

As the probability of choosing both coins is equal, we know that the $\pi_k=0.5$ in both cases. Thus, we can eliminate it from our expression. Also, we fix the $\Theta_k$ values (first iteration uses randomly initiated values, while the subsequent steps use values calculated in the previous step) in the E step of the EM algorithm. 

In the M step, we fix the expectation value calculated in the E step and vary $\Theta_k$ values. To find the values which maximize the expectation of log equation, we use the derivation. Second term, after the summation sign becomes constant, so it does not have the effect on the maximization procedure.
\begin{equation}
maxL(\Theta)=E_{Z|X}[p(X_1,\dots, X_5,{z_1,\dots,z_5}|\Theta]
\end{equation}
\begin{equation}
=\sum_{n=1}^{5}\sum_{j=1}^{10}\sum_{k=1}^{2}E_{Z|X}[z_{nk}]log(\Theta_k^{x_n^j}(1-\Theta_k^{1-x_n^j}))+C
\end{equation}
\begin{equation}
=\sum_{n=1}^{5}\sum_{j=1}^{10}\sum_{k=1}^{2}E_{Z|X}[z_{nk}]({x_n^j}log(\Theta_k)+({1-x_n^j})log(1-\Theta_k))+C
\end{equation}
\\
For a single coin $k$
\begin{equation}
\frac{dL(\Theta)}{d\Theta_k}=\sum_{n=1}^{5}\sum_{j=1}^{10}E_{Z|X}[z_{nk}](\frac{x_n^j}{\Theta_k}-\frac{1-x_n^j}{1-\Theta_k})=0
\end{equation}
Multiplying with both denominators $\Theta_k(1-\Theta_k)$ we get
\begin{equation}
\frac{dL(\Theta)}{d\Theta_k}=\sum_{n=1}^{5}\sum_{j=1}^{10}E_{Z|X}[z_{nk}]({x_n^j}-{\Theta_k})=0
\end{equation}
and finally, since $\Theta$'s index does not appear in the summation indexes, we can simply divide to obtain its (maximal) value
\begin{equation}
\Theta_k=\frac{\sum_{n=1}^{5}\sum_{j=1}^{10}E_{Z|X}[z_{nk}]{x_n^j}}{10\:\sum_{n=1}^{5}E_{Z|X}[z_{nk}]}
\end{equation}
We see that the result for each coin is weighted expectation of a single coin over a sum of the expectations of both coins.

EM is sort of like trying to move a heavy table without anyone's help. You push a bit from one side and then from the other. Bit by bit you are reaching the goal.

\section{EM algorithm}
%\textbf{Input:} Data x_{1:n}, number of components K
%\textbf{Output:} Optimal parameters $\Theta$
%\textbf{Initialize:} Random parameters $\Theta$=$\Theta$_{1:K}, threshold T
%\textbf{while} parameters have not converged \textbf{do}
%	Compute E[z_{nk}] for all possible hidden variables
%	Compute $\Theta$_{k}^{new} by maximizing E[z_{nk}]
%	Compute ConvergenceCriteria
%	Assign $\Theta$=$\Theta$^{new}



\section{Results}
In order to verify our result we have compared it to ~\cite{nature} where the problem is originally described. We chose the ssame values $\Theta$=[0.6, 0.5] and we iterate EM steps until convergence. Here we define convergence in a following way. We choose a threshold T. EM procedure is continued until change of either of $\Theta_k$ in subsequent iterations is greater than value T. 



\begin{thebibliography}{} 
\bibitem{wikiEM} Expectation Maximization algorithm wiki page
\bibitem{nature} Chuong B Do,  Serafim Batzoglou, \emph{What is the expectation maximization algorithm}, Nature Biotechnology 26, 897-899, dpi:10.1038/nbt14052008, \url{http://www.nature.com/nbt/journal/v26/n8/full/nbt1406.html}

\end{thebibliography}

\end{document}
